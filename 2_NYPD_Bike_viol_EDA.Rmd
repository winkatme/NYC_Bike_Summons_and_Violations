---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r libraries, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
library(tidyverse)
library(lubridate)
knitr::opts_chunk$set(message = FALSE)
# options(digits = 7)
library(rmarkdown)
library(fpp3)
#library(forecast)
library(readxl)
library(httr)
library(glue)
# install.packages("corrr")
library(corrr)
library(slider)
library(forcats)
# devtools::install_github('smin95/smplot2', force = TRUE)
library(smplot2)
# install.packages('sf')
library(sf)
# install.packages('Rcpp')
library(Rcpp)
# install.packages('mapview')
library(mapview)
library(janitor)
#install.packages('styler')
library(styler)
#install.packages('patchwork')
library(patchwork)
```

# Import Data

```{r import-and-tsibble, include=FALSE}
df_bike_violations <- readr::read_csv("Data/Processed/df_bike_violations_2023-08-05.csv")

df_bike_violations <- df_bike_violations |>
  mutate(violation_code = as.factor(violation_code)) |>
  mutate(veh_category = as.factor(veh_category)) |>
  mutate(city_nm = as.factor(city_nm)) |>
  mutate(rpt_owning_cmd = as.factor(rpt_owning_cmd)) |>
  mutate(violation_date = as.POSIXct(violation_date))
```

# Create and clean daily tsibble for time series analysis

```{r clean, message=FALSE, warning=FALSE}
ts_daily_total <- df_bike_violations |>
  mutate(violation_date = as_date(violation_date)) |>
  group_by(violation_date, daily_total_cyclists) |>
  summarize(daily_total_violations = sum(n())) |>
  ungroup() |>
  as_tsibble(index = violation_date)

# check for missing dates  # ans: 24
ts_daily_total |>
  # filter(is.na(daily_total_cyclists)) |>
  count_gaps() |> 
  nrow()
```

```{r clean2, eval=FALSE}
# are the days missing from df_counts?  # ans: no.
# df_counts |>
#  filter(is.na(daily_total))
```

Fill gaps in dates and re-join with df_counts data, so that gapped dates now have total cyclists columns. Replace violation na's with 0's.

```{r clean3, warning=FALSE, message=FALSE}
# import df_counts:
path3 <- "https://www.dropbox.com/s/nffkpwuz1u6kd5t/df_counts.csv?dl=1"
df_counts <- readr::read_csv(path3)

# fill date gaps for ts_daily_total and join with df_counts
ts_daily_total <- ts_daily_total |>
  fill_gaps() |>
  select(-daily_total_cyclists) |>
  mutate(date = violation_date) |>
  left_join(y = df_counts, by = "date", multiple = "any") |>
  rename(daily_total_cyclists = daily_total) |>
  select(-c("date", "counts", "weekly_total")) |>
  mutate_at("daily_total_violations", ~ replace_na(., 0))

# double check for na's:
# ts_daily_total |>
#  filter(is.na(daily_total_cyclists))

# double check for gaps:
ts_daily_total |> 
  count_gaps()
```

## Optional: Create st/sf for spatial analysis

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# WORK IN PROGRESS

# import df
# source for help:  https://gis.stackexchange.com/questions/305403/load-a-csv-as-simple-features-r

st_bike_violations <- readr::read_csv("Data/Processed/df_bike_violations_2023-08-05.csv") |> 
  filter(!is.na(longitude)) |> 
  st_as_sf(coords=c("longitude","latitude"), crs=4326) # remember x=lon and y=lat

class(st_bike_violations$geometry)

st_bbox(st_bike_violations)

st_crs(st_bike_violations)

#install.packages('stars')
library(stars)

plot(st_bike_violations)

# point pattern test
#install.packages('spatstat')
library(spatstat)

ppp_bike_violations <- df_bike_violations |> 
  mutate(date = date(violation_date)) |> 
  filter(date >= '2022-04-01') |> 
  filter(violation_code=='1111D1C') |> 
  filter(!is.na(x_coord_cd)) |> 
  st_as_sf(coords = c("x_coord_cd", "y_coord_cd")) |> 
  as.ppp()

plot(ppp_bike_violations$window)
plot(ppp_bike_violations)

#plot(density(ppp_bike_violations,sigma = bw.diggle))
#plot(ppp_bike_violations, add=T)


# test: clusters for st_bike_violations

?hclust



d = dist(x = st_bike_violations$location, method = 'euclidean') 

dim(st_bike_violations)


st_bike_violations |> 
  select(x_coord_cd, y_coord_cd) |> 
  as.matrix() |> 
  dist()
# note: chnaged environment:
# source: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
#library(usethis) 
#usethis::edit_r_environ()

# since vector was too large to make distance matrix, just choose 1111D1C, since april 2022

st_bike_violations_1111D1C <- st_bike_violations |> 
  mutate(date= date(violation_date)) |> 
  filter(date >= "2022-04-01") |>
  filter(violation_code=='1111D1C')

d = dist(x = st_bike_violations_1111D1C$location, method = 'euclidean')

clusters = hclust(d = d,method='ward.D2')



# test for spdep
#install.packages('spdep')
library(spdep)

st_bike_violations_1111D1C |> 
  st_geometry() |> 
  plot(border = "grey", lwd = 0.5)
nb_soi |> plot(coords = coords, add = TRUE, 
               points = FALSE, lwd = 0.5)
```


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# WORK IN PROGRESS
# test for ppp point pattern analysis 
# source: https://cran.r-project.org/web/packages/spatstat/vignettes/getstart.pdf

# for binding box dimensions:
st_bbox(st_bike_violations_1111D1C)

st_crs(st_bike_violations_1111D1C)
owin(st_geometry(st_bike_violations_1111D1C))
?st_transform()

?owin

# this works
ppp_test <- ppp(st_coordinates(st_bike_violations_1111D1C)[,1], st_coordinates(st_bike_violations_1111D1C)[,2], c(-74.1556, -73.76750), c(40.57416, 40.88731))
plot(ppp_test)



st_bike_violations_1111D1C

ppp_test

# work:
ggplot() + 
  geom_sf(data = st_bike_violations_1111D1C, size=0.1)


# this works
plot(density(ppp_test))

plot(quadratcount(ppp_test, nx=10, ny=10))


kppm_test<-kppm(ppp_test)
plot.kppm_test

ppp_clust<-clusterset(ppp_test)

plot(ppp_test, main=ppp_clust)
ppp_clust

as.SpatialPoints.ppp(ppp_test)
ppp_test$y

mapview(ppp_test, xcol = ppp_test$x, ycol = ppp_test$y, crs = 4269, grid = FALSE, alpha.regions = 0.1)

```

```{r}
# WORK IN PROGRESS

# cluster test with regular data
df_bike_violations_1111D1C <- df_bike_violations |> 
  mutate(date = date(violation_date)) |> 
  filter(date >= '2022-04-01') |> 
  filter(violation_code=='1111D1C')

colnames(df_bike_violations_1111D1C)

distances <- df_bike_violations_1111D1C |> 
  select(longitude, latitude) |> 
  dist(method='euclidean')

distances


clust = hclust(distances,method = "ward.D2")
plot(clust)

#library(dendextend)

clusters = cutree(clust, k=6)
clusters

density <- density(ppp_test, sigma = 0.005)
plot(density)

str(density)

install.packages('tigris')
library(tigris)
manhattan_tigris = tracts(state='NY',county = 'New York',cb = TRUE)

#install.packages('tmap')
library(tmap)

tmap_mode("view")

tm_shape(manhattan_tigris)+
  tm_borders()+
  plot(density)


# ggmaps
?register_google 

library(ggmap)
map = get_map(location = c(-77.08597,38.7),source="google",maptype="roadmap",zoom=12)
ggmap(map)+geom_point(data = dat2,aes(x=longitude,y=latitude,size=Magnitude))

ggplot(data=density$v)

install.packages('rgdal')
library(rgdal)

class(nyc)

?density

```



## Create monthly and yearly total data tsibbles

```{r monthly-tsibble}
# create a monthly data tsibble from cleaned daily tsibble
ts_monthly_total <- ts_daily_total |>
  index_by(yearmonth(violation_date)) |>
  summarize(across(c("daily_total_violations", "daily_total_cyclists"), ~ sum(.x, na.rm = TRUE))) |>
  rename(monthly_total_cyclists = daily_total_cyclists) |>
  rename(monthly_total_violations = daily_total_violations) |>
  rename(date = "yearmonth(violation_date)")

ts_yearly_total <- ts_daily_total |>
  index_by(year(violation_date)) |>
  summarize(across(c("daily_total_violations", "daily_total_cyclists"), ~ sum(.x, na.rm = TRUE))) |>
  rename(yearly_total_cyclists = daily_total_cyclists) |>
  rename(yearly_total_violations = daily_total_violations) |>
  rename(date = "year(violation_date)")

```

# EDA: General

## What's the date range for the data?

```{r}
date(range(df_bike_violations$violation_date))
```

## Which dates had the highest rider counts?

```{r top-10-daily-total-cyclist-days}
df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  arrange(desc(daily_total_cyclists)) |>
  distinct(violation_date, daily_total_cyclists) |>
  slice(1:10) |>
  kable(caption = "Top 10 busiest cyclist days", align = "c")
```

Takeaway: Busiest single days were mostly in the autumn of 2019.

## Which dates had the highest number of violations, and how many violations?

```{r}
df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  summarize(
    daily_total_violations = sum(n()),
    .by = violation_date
  ) |>
  arrange(desc(daily_total_violations)) |>
  slice(1:10) |>
  kable(caption = "Top 10 busiest violation days", align = "c")
```

Takeaway: mostly warmer days in 2018 and 2019, pre-pandemic.

# EDA: Bike Counters

## Total bike counters

```{r, message=FALSE}
df_bicycle_counters_boroughs <- readr::read_csv("Data/Processed/df_bicycle_counters_boroughs.csv")

nrow(df_bicycle_counters_boroughs)
```

## Geoplot: bike counters

```{r geoplot-bike-counters}
mapview(na.omit(df_bicycle_counters_boroughs), xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)
```

## Bike counters per borough

```{r bike-counters-per-borough}
df_bicycle_counters_boroughs |> 
  group_by(borough) |> 
  count()
```

Takeaway: mostly in Manhattan. Even the ones in the Bronx and SI are located near Manhattan access points.

## Bike counters used per year

```{r, rows.print = 12}
df_counts |> 
  mutate(id = as.factor(id)) |> 
  group_by(year=year(date)) |> 
  distinct(id) |> 
  summarise(n=n()) 
```

Takeway: It looks like max per year is 14. so not all are 18 used at once.

## Which bike counters counted the most cyclists in 2022?

```{r, message=FALSE}
df_counts |> 
  mutate(date = date(date)) |>
  filter(year(date)==2022) |> 
  group_by(id, year(date)) |> 
  summarize(sum = sum(counts)) |> 
  arrange(desc(sum))
  
# geoplot
#df_bike_violations_bicycle_counters |> filter(id==300020904) |> mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)  
```

Takeaway:\
Top 5:\
1. 100009427 with 1964902: Williamsburg side of Williamsburg bridge.\
2. 100009428 with 1818163: Queens side of Queensboro Bridge\
3. 100047029 with 1584788: Manhattan side of Manhattan bridge\
4. 100010019 with 1066870: Kent ave in Wburg\
5. 300020904 with 1002070: Brooklyn Bridge

## Which bike counters counted the most cyclists in 2023, so far?

```{r}
df_counts |> 
  mutate(date=year(date)) |> 
  filter(date=='2023') |> 
  mutate(id=as.factor(id)) |> 
  summarize(sum=sum(counts), .by=id) |> 
  arrange(desc(sum))
```

Takeaway: Top 3 are the same. 4 and 5 are switched

```{r top-three-counters-2022-2023-names}
# get names of top 3 counters
df_bicycle_counters_boroughs |> 
  filter(id %in% c(100009427, 100009428, 100047029))
```

## Time series plot for Wburg bridge bike path bike counter

Slope of wburg daily ridership line since 2018

```{r message=FALSE, warning=FALSE}
df_counts |> 
  filter(id==100009427) |> 
  mutate(date = date(date)) |> 
  filter(year(date) >= 2018) |> 
  group_map(~ broom::tidy(lm(daily_total ~ date, data = .x))) 
  
```

Takeaway: 6.5 riders per day increase for wburg bridge.

Plot:

```{r Plot-wburg-bridge-daily-counts, message=FALSE}
df_counts |> 
  filter(id==100009427) |> 
  mutate(date = date(date)) |> 
  filter(year(date) >= 2018) |> 
  distinct(date, .keep_all = TRUE) |> 
  ggplot(aes(x=date, y=daily_total))+
  geom_line()+
  geom_smooth(method = lm)+
  #geom_abline(slope = coef(lm(daily_total ~  date(date), data=df_counts))[2], intercept = coef(lm(daily_total ~  date(date), data=df_counts))[1])+
  labs(title="Williamsburg bridge bike path daily counts since 2018")
```

Takeaways: \* Seasonality trend - fewer riders in the colder months, more in the summer months. (Note that when graphing all time points since 2014, there was a big increase in the winter of 2016. This was due to the new citibikes, as well as an unusually warm winter). \* Ridership growth since 2018 is \~6.5 increase per day. \* Big bump in the fall of 2019. Bump in the summer after covid lockdown, Slight dip in the summr of 2021, but rising again in 2022.

# EDA: Violation Codes

## How many unique violations were handed out to cyclist?

```{r}
length(fct_unique(df_bike_violations$violation_code))
```

## How many total violations were handed out to cyclists since 2018?

```{r}
length(df_bike_violations$violation_code)
```

## What were the most common violations?

```{r 10-most-common-violations-table, message=FALSE}
df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  summarize(
    total = sum(n()),
    .by = c(violation_code, description)
  ) |>
  arrange(desc(total)) |>
  mutate(percent = 100 * round(total / sum(total), 3)) |>
  slice(1:10) |>
  kable()

# note:  12332: ATTACHING SELF TO MOVING MOTOR VEHICLE a/k/a, the Marty McFly rule
```

Takeaway: 44.1% + 5.2% ≈ 50% were for failing to stop at a red light.

### Unique violation: code 12332: Marty McFly

```{r Marty-McFly}
df_bike_violations |> 
  filter(violation_code==12332) |> 
  select(violation_code, violation_date, description)

df_bike_violations |> 
  filter(violation_code==12332) |> 
  mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)
```

## Plot: Total violations per year, per borough.

```{r plot-Yearly-Cyclist-Violations}
df_bike_violations |> 
  group_by(year=year(violation_date), city_nm) |> 
  ggplot(aes(x=year, fill=city_nm))+
  geom_bar()+
  #geom_bar(position = position_dodge())+
  labs(title="Yearly Cyclist Violations", fill="Borough")+
  scale_fill_brewer()
```

Takeaway: Large drop across the board in violations after 2019, especially in Manhattan. Barely any in Staten Island

## Geocode: Violations location clusters

```{r}
# eomap of violations, post-covid
# wasnt able to use dplyr due to missing values.
df_fct_lump_post_covid <- df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2020-04-01")


mapview(na.omit(df_fct_lump_post_covid), xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.1)
```

map is too full, let's just view since April 2022

```{r}
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-04-01") |>
  mapview(xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.2)
```

better, but still not too clear.

How about just one month - May 2022?

```{r geoplot-vilations-may-2022}
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-05-01" & date <= "2022-05-31") |> 
  mapview(xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.2)
```

What were the total number of violations in this time frame, May of 2022?

```{r}
df_bike_violations |> 
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-05-01" & date <= "2022-05-31") |>   
  nrow()
```

For the sake of clustering, let's just view the most common violation, 11111D1C, for the previous year of data (since April of 2022):

```{r geomap-most-common-voilation-past-year}
# Let's just view fct_lump==1111D1C for the previous year:
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-04-01") |>
  filter(fct_lump == "1111D1C") |>
  mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE, alpha.regions = 0.2)

```

What were the total number of 1111D1C violations in this time frame?

```{r}
df_bike_violations |> 
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-04-01") |>
  filter(fct_lump == "1111D1C") |>
  nrow()
```

Takeaway: We have a better view of the major ticketing areas for 1111D1C: \* UWS \* UES (and their corresponding avenues up through Harlem), \* Up and down 1st and 2nd aves, especially in the east village, \* On the Brooklyn side of the Williamsburg Bridge, \* Up and down 4th and 5th aves in Brooklyn, \* Liberty Ave in Ozone park/ Richmond Hill, \* The East side of Prospect Park on Bedford Ave \* Bensonhurst, bk.

## Is there a trend for 1111D1C over time?

```{r}
df_bike_violations |> 
  mutate(date = date(violation_date)) |>
  filter(violation_code=="1111D1C") |> 
  group_by(date) |> 
  count() |> 
  ggplot(aes(x=date, y=n))+
  geom_line()
```

Takeaway: the trend does not look out of the ordinary.

# EDA: Daily Cyclists

## Plot: daily total cyclists over time

```{r message=FALSE }
ts_daily_total |>
  autoplot(daily_total_cyclists) +
  geom_smooth(method = "lm")
```

Takeaway: ridership did not change much pre or post pandemic. not even during lockdown, although possible hiccup. However, ridership seems to be increasing in the colder months while remaining level in the warmer months. Since the ridership is clearly cyclical, an average won't tell us as much as we like. But we can still see an increase over time.

### Slope: Daily increase in number of cyclists

```{r message=FALSE }
coef(lm(ts_daily_total$daily_total_cyclists~ts_daily_total$violation_date))[2]

```

## Plot: yearly trend in number of cyclists, to 2022

```{r message=FALSE }
plot1<-df_counts |> 
  #mutate(date = mdy_hms(date)) |> 
  group_by(date=year(date)) |> 
  summarise(yearly = sum(counts)) |> 
  filter(date < 2023) |> 
  ggplot(aes(x=date, y=yearly))+
  geom_line()+
  scale_x_continuous(breaks=seq(2012, 2022,2))+
  labs(title='Yearly total cyclists', y='Cyclists')


plot2 <-df_counts |> 
  mutate(id = as.factor(id)) |> 
  group_by(year=year(date)) |> 
  distinct(id) |> 
  summarise(n=n()) |> 
  ggplot(aes(x=year, y=n))+
  geom_line()+
  scale_x_continuous(breaks=seq(2012, 2022,2))+
  labs(title='Bike Counters per Year', y='Bike Counters')

plot1 + plot2
```

This isn't too accurate early on, as there were fewer bike counters used.

## Histogram: daily total cyclists

```{r}
ts_daily_total |>
  ggplot(aes(x = daily_total_cyclists)) +
  geom_histogram(bins = 80)
```

## Does ridership increase or decrease yearly?

```{r plot-yearly-ridership}
# leave out 2023 since we only have up to March

ts_yearly_total |> 
  filter_index(. ~ "2022") |>
  autoplot(yearly_total_cyclists)
```

Takeaway: Yes, it increases. but at a slower rate after covid

## Does ridership change throughout the year (seasonality)?

```{r}
# yearly seasonal plot
ts_daily_total |>
  gg_season(daily_total_cyclists)
```

Takeaway - it's roughly the same yearly cycle per year. increase in ridership from May-Oct, decrease Nov-Apr

A plot with monthly totals would look cleaner while still conveying enough information

```{r plot-monthly-totals}
ts_monthly_total |>
  gg_season(monthly_total_cyclists)
```

Takeaway - dip in April 2020 during covid lockdown, but not much less than before.

```{r yearly-ridership-trends-by-month}
ts_monthly_total |>
  gg_subseries(monthly_total_cyclists)
```

Takeaways: \* Seasonality is clearer. Monthly differences are clearer. \* 2023 is turning out to be noticeably more riders than previous years in colder months of Jan and Feb.

Ridership changes over time takeaways: \* Ridership is increasing. \* Clear yearly seasonality, with May-Oct being the busiest, and Nov-Apr being less busy.

## Daily total cyclists decomposition

### Classical Decomp

```{r classical-decomp, message=FALSE, warning=FALSE }
ts_daily_total |>
  #  filter(!is.na(daily_total_cyclists)) |>
  #  fill_gaps() |>
  model(classical_decomposition(daily_total_cyclists, type = "additive")) |>
  components() |>
  autoplot()
```

Takeaway: Classical decomposition does not handle multi-seasonality of the data too well.

### STL Decomp

```{r stl-decomp, message=FALSE, warning=FALSE }
ts_daily_total |>
  model(
    STL(daily_total_cyclists ~ trend(window = 365) +
      season(period = "year") + season(period = "1 month"), )
  ) |>
  components() |>
  autoplot()
```

Takeway: STL is best

### Moving average

```{r moving-average, message=FALSE, warning=FALSE }
# what about moving average for trend?  it ok.  I think stl is best.
ts_daily_total |>
  mutate(`5-MA` = slider::slide_dbl(daily_total_cyclists, mean, .before = 90, .after = 90, .complete = TRUE)) |>
  autoplot(daily_total_cyclists) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00")
```

# EDA: Daily Violations

## Plot: daily total violations over time

```{r plot-daily-total-violations}
ts_daily_total |>
  autoplot(daily_total_violations)
```

Takeaway: There were more violations before the pandemic. During lockdown, very few.

## Histogram: daily total violations

```{r}
ts_daily_total |>
  ggplot(aes(x = daily_total_violations)) +
  geom_histogram(bins = 80)
```

## Are the yearly/monthly number of violations trending up or down?

```{r plot-yearly-total-violations}
ts_yearly_total |> 
  filter_index(. ~ "2022") |>
  autoplot(yearly_total_violations)+
  ylim(0, NA)
```

Takeaway: large drop in violations after lockdown, but staying steady.

```{r plot-monthly-total-violations}
# monthly:  just cleaner graph of daily violations over time.  Post-covid looks steady.
ts_monthly_total |>
  autoplot(monthly_total_violations)
```

Takeaway: Some seasonality - more violations in warmer months, dips to fewer in colder. This seasonality was less pronounced during 2020, but seems to be going back to previous trends in 2022

## Do the number of violations change throughout the year (seasonal)?

Let's look at just post-lockdown:

```{r plot-viol-seasonality-post-lockdown}
ts_daily_total |>
  filter_index("2020-03" ~ .) |>
  gg_season(daily_total_violations)
```

Monthly is easier to see

```{r plot-viol-seasonality-monthly-post-lockdown}
ts_monthly_total |>
  filter_index("2020-03" ~ .) |>
  gg_season(monthly_total_violations)
```

Let's view a subseries plot

```{r plot-viol-seasonality-subseries-post-lockdown}
ts_monthly_total |>
  filter_index("2020-03" ~ .) |>
  gg_subseries(monthly_total_violations)
```

Takeawy: Hard to see any trends, but fewer violations in Nov/Dec/Jan/Feb. Most in Sept, March, August. April has a lower average due to covid lockdown in 2020.

## As the number of riders increases, does the number of violations also increase?

```{r message=FALSE}
# utilizes corrr library

df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  group_by(violation_date) |>
  mutate(daily_total_violations = sum(n())) |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() 
```

Takeway: cor: 0.0768097 So no, as the number of riders increases, the number of violations does not.

## However, Does this differ for pre-covid and post-lockdown?

### Plot: daily violations vs. daily cylists, since 2018

```{r}
ts_daily_total |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point()
```

### Plot: daily violations vs. daily cylists, Pre-lockdown

```{r message = FALSE, warning=FALSE}
# correlation:  0.46
cor <- ts_daily_total |>
  filter_index(~"2020-04") |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() |>
  slice(1) |>
  select(daily_total_violations) |>
  as.numeric()

# pre-covid
ts_daily_total |>
  filter_index(~"2020-04") |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point() +
  geom_smooth(method = lm) +
  sm_statCorr(label_x = 250, label_y = 1000)
```

### Plot: daily violations vs. daily cylists, Pose-lockdown

```{r message=FALSE}
# devtools::install_github('smin95/smplot2', force = TRUE)
# library(smplot2)

# post-covid
ts_daily_total |>
  filter_index("2020-04" ~ .) |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point() +
  geom_smooth(method = lm) +
  sm_statCorr(label_x = 120, label_y = 1000)

# correlation:  ~0.37
ts_daily_total |>
  filter_index("2020-04" ~ .) |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() |>
  kable()

# ans: they differ a little more than all together.  pre-covid was a better indicator.
```

Takeaway: Yes, there is a positive correlation between cyclists per day and violations per day, when the data is split pre- and post-lockdown. It's strength is medium though, and does not tell the whole picture.

## Do largest violations (summed per day) change over time?

```{r message = FALSE}
# plot: daily biggest violation (1111D1C) over time, post covid
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  filter(fct_lump == "1111D1C") |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2020-04-01") |>
  summarize(
    sum = sum(n()),
    .by = c(date, fct_lump)
  ) |>
  ggplot(aes(x = date, y = sum)) +
  geom_line()
```

Takeaway: No. not much different than general daily violations plot. lower amount around Dec/Jan, otherwise fairly steady.

## To Do: Did the type of violations change after covid?

```{r}


```

## To do: Can we forecast ridership?

Let's see how the daily totals look

```{r}
ts_daily_total |> 
  autoplot(daily_total_cyclists)
```

Pretty messy.  The monthly totals look easier to read, so let's use those.

```{r}
ts_monthly_total |> 
  autoplot(monthly_total_cyclists)
```


### Train and test sets for monthly data

```{r}
train<-ts_monthly_total |> 
  select(!monthly_total_violations) |> 
  filter_index(~"2021-12")
test<-ts_monthly_total |> 
  select(!monthly_total_violations) |> 
  filter_index("2022-01"~.)
```

### Seasonal Naive forecast on monthly totals

```{r}
ts_monthly_snaive_mod <- ts_monthly_total |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12)))

ts_fc <- ts_monthly_snaive_mod|> forecast(h = "2 years")

ts_fc |> 
  autoplot(ts_monthly_total)

```


#### Seasonal naive monthly residuals

```{r, warning=FALSE}
train_snaive_mod <- train |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12)))

mean(augment(train_snaive_mod)$.resid, na.rm=TRUE)  # 85885.5

gg_tsresiduals(train_snaive_mod)
```

Tests to see if residuals differ from white noise:

```{r}
augment(train_snaive_mod) %>% features(.innov, box_pierce, lag = 12)
augment(train_snaive_mod) %>% features(.innov, ljung_box, lag = 12)
```

Takeaways: passes box_pierce, but almost passes ljung_box.  Resids don't look close to normal.  

How does it do on the test set?

```{r}

fct <-train |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12))) |> 
  forecast(test)

(RMSE_seasonal_naive_monthly <-accuracy(fct, ts_monthly_total)$RMSE)

```

RMSE 131213.6	

#### Visualize the Seasonal Naive forecast:

```{r message=FALSE, warning=FALSE}
autoplot(train) +
  autolayer(fct, PI = F, size=0.7)+
  autolayer(test)
```

Takeaway:  Errors can get extreme (less than 0 at one point). Does not take into account the trend of the data.  Let's compare to other methods.





## To do: Can we foreast daily violations?


```{r scratchpad, include=FALSE}


str(df_bike_violations)

# plot shows that ebikes and escooters didnt have their own categories until sometime in mid 2022. before that, everything was under 'bikes'
df_bike_violations |>
  ggplot(aes(x = violation_date, y = city_nm, col = veh_category)) +
  geom_jitter()

# nuthin
df_bike_violations |>
  ggplot(aes(x = violation_date, y = city_nm, col = violation_code)) +
  geom_jitter()

# violations per month
df_bike_violations |>
  group_by(yearmonth = yearmonth(violation_date)) |>
  summarize(total_violations = sum(n())) |>
  ggplot(aes(x = yearmonth, y = total_violations)) +
  geom_line()


# violations per borough per year
df_bike_violations |>
  group_by(year = yearmonth(violation_date), city_nm) |>
  summarize(total_violations = sum(n())) |>
  ggplot(aes(x = year, y = total_violations, col = city_nm)) +
  geom_line()


# cyclist counters per borough:
# library(forcats)
# df_bicycle_counters_boroughs |>
#  ggplot(aes(x = fct_infreq(Borough))) +
#  geom_bar() +
#  labs(x = "Borough", y = "Cyclist Counters")
```

```{r}

```
