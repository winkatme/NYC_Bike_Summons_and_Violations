---
title: "R Notebook"
output:
  html_document:
    df_print: paged
---

```{r libraries, message=FALSE, warning=FALSE, results='hide'}
library(knitr)
library(tidyverse)
library(lubridate)
knitr::opts_chunk$set(message = FALSE)
# options(digits = 7)
library(rmarkdown)
library(fpp3)
#library(forecast)
library(readxl)
library(httr)
library(glue)
# install.packages("corrr")
library(corrr)
library(slider)
library(forcats)
#install.packages('devtools')
#devtools::install_github('smin95/smplot2', force = TRUE)
library(smplot2)
# install.packages('sf')
library(sf)
# install.packages('Rcpp')
library(Rcpp)
# install.packages('mapview')
library(mapview)
library(janitor)
#install.packages('styler')
library(styler)
#install.packages('patchwork')
library(patchwork)
#install.packages('kableExtra')
library(kableExtra)
#install.packages("finalfit")
library(finalfit)
```

# Import Data

```{r import, include=FALSE}
df_bike_violations <- readr::read_csv("Data/Processed/df_bike_violations_2023-09-16.csv")

df_bike_violations <- df_bike_violations |>
  mutate(violation_code = as.factor(violation_code)) |>
  mutate(veh_category = as.factor(veh_category)) |>
  mutate(city_nm = as.factor(city_nm)) |>
  mutate(rpt_owning_cmd = as.factor(rpt_owning_cmd)) |>
  mutate(violation_date = as.POSIXct(violation_date))
```

# Create and clean daily tsibble for time series analysis

Fill date gaps, count total violations per day, and re-join daily counts data to fill in missing daily rider count from the former date gaps.

```{r daily-tsibble, message=FALSE, warning=FALSE}

# import and clean df_counts:
path3 <- "https://www.dropbox.com/s/nffkpwuz1u6kd5t/df_counts.csv?dl=1"
df_counts_dl <- readr::read_csv(path3)

str(df_counts_dl)


# double-check 18 distinct counter ids.  I was having issues saving 
# df_counts and opening it without duplicate bike counters.
df_counts_dl |> 
  distinct(id)

df_counts <-df_counts_dl |> 
  group_by(date(date)) |>
  mutate(daily_total = sum(counts)) |>
  ungroup() |>
  group_by(yearweek(date)) |>
  mutate(weekly_total = sum(counts)) |>
  ungroup() |>
  arrange(date) |>
  select(!c("date(date)", "yearweek(date)", countid, status))


# create the daily counts tsibble
ts_daily_total <- df_bike_violations |> 
  group_by(violation_date = as_date(violation_date)) |> 
  summarise(daily_total_violations = n()) |> 
  as_tsibble(index = violation_date) |> 
  fill_gaps() |> 
  mutate(date = violation_date) |> 
  left_join(y = df_counts, by = "date", multiple = "any") |> 
  rename(daily_total_cyclists = daily_total)|> 
  select(-c(date, id, counts, weekly_total))

ts_daily_total |> 
  autoplot(daily_total_violations)

```


## Optional: Create st/sf for spatial analysis

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# WORK IN PROGRESS

# import df
# source for help:  https://gis.stackexchange.com/questions/305403/load-a-csv-as-simple-features-r

st_bike_violations <- readr::read_csv("Data/Processed/df_bike_violations_2023-08-05.csv") |> 
  filter(!is.na(longitude)) |> 
  st_as_sf(coords=c("longitude","latitude"), crs=4326) # remember x=lon and y=lat

class(st_bike_violations$geometry)

st_bbox(st_bike_violations)

st_crs(st_bike_violations)

#install.packages('stars')
library(stars)

plot(st_bike_violations)

# point pattern test
#install.packages('spatstat')
library(spatstat)

ppp_bike_violations <- df_bike_violations |> 
  mutate(date = date(violation_date)) |> 
  filter(date >= '2022-04-01') |> 
  filter(violation_code=='1111D1C') |> 
  filter(!is.na(x_coord_cd)) |> 
  st_as_sf(coords = c("x_coord_cd", "y_coord_cd")) |> 
  as.ppp()

plot(ppp_bike_violations$window)
plot(ppp_bike_violations)

#plot(density(ppp_bike_violations,sigma = bw.diggle))
#plot(ppp_bike_violations, add=T)


# test: clusters for st_bike_violations

?hclust



d = dist(x = st_bike_violations$location, method = 'euclidean') 

dim(st_bike_violations)


st_bike_violations |> 
  select(x_coord_cd, y_coord_cd) |> 
  as.matrix() |> 
  dist()
# note: chnaged environment:
# source: https://stackoverflow.com/questions/51295402/r-on-macos-error-vector-memory-exhausted-limit-reached
#library(usethis) 
#usethis::edit_r_environ()

# since vector was too large to make distance matrix, just choose 1111D1C, since april 2022

st_bike_violations_1111D1C <- st_bike_violations |> 
  mutate(date= date(violation_date)) |> 
  filter(date >= "2022-04-01") |>
  filter(violation_code=='1111D1C')

d = dist(x = st_bike_violations_1111D1C$location, method = 'euclidean')

clusters = hclust(d = d,method='ward.D2')



# test for spdep
#install.packages('spdep')
library(spdep)

st_bike_violations_1111D1C |> 
  st_geometry() |> 
  plot(border = "grey", lwd = 0.5)
nb_soi |> plot(coords = coords, add = TRUE, 
               points = FALSE, lwd = 0.5)
```


```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# WORK IN PROGRESS
# test for ppp point pattern analysis 
# source: https://cran.r-project.org/web/packages/spatstat/vignettes/getstart.pdf

# for binding box dimensions:
st_bbox(st_bike_violations_1111D1C)

st_crs(st_bike_violations_1111D1C)
owin(st_geometry(st_bike_violations_1111D1C))
?st_transform()

?owin

# this works
ppp_test <- ppp(st_coordinates(st_bike_violations_1111D1C)[,1], st_coordinates(st_bike_violations_1111D1C)[,2], c(-74.1556, -73.76750), c(40.57416, 40.88731))
plot(ppp_test)



st_bike_violations_1111D1C

ppp_test

# work:
ggplot() + 
  geom_sf(data = st_bike_violations_1111D1C, size=0.1)


# this works
plot(density(ppp_test))

plot(quadratcount(ppp_test, nx=10, ny=10))


kppm_test<-kppm(ppp_test)
plot.kppm_test

ppp_clust<-clusterset(ppp_test)

plot(ppp_test, main=ppp_clust)
ppp_clust

as.SpatialPoints.ppp(ppp_test)
ppp_test$y

mapview(ppp_test, xcol = ppp_test$x, ycol = ppp_test$y, crs = 4269, grid = FALSE, alpha.regions = 0.1)

```

```{r message=FALSE, warning=FALSE, include=FALSE}
# WORK IN PROGRESS

# cluster test with regular data
df_bike_violations_1111D1C <- df_bike_violations |> 
  mutate(date = date(violation_date)) |> 
  filter(date >= '2022-04-01') |> 
  filter(violation_code=='1111D1C')

colnames(df_bike_violations_1111D1C)

distances <- df_bike_violations_1111D1C |> 
  select(longitude, latitude) |> 
  dist(method='euclidean')

#distances


clust = hclust(distances,method = "ward.D2")
plot(clust)

#library(dendextend)

clusters = cutree(clust, k=6)
clusters

density <- density(ppp_test, sigma = 0.005)
plot(density)

str(density)

install.packages('tigris')
library(tigris)
manhattan_tigris = tracts(state='NY',county = 'New York',cb = TRUE)

#install.packages('tmap')
library(tmap)

tmap_mode("view")

tm_shape(manhattan_tigris)+
  tm_borders()+
  plot(density)


# ggmaps
?register_google 

library(ggmap)
map = get_map(location = c(-77.08597,38.7),source="google",maptype="roadmap",zoom=12)
ggmap(map)+geom_point(data = dat2,aes(x=longitude,y=latitude,size=Magnitude))

ggplot(data=density$v)

install.packages('rgdal')
library(rgdal)

class(nyc)

?density

```



## Create monthly and yearly total data tsibbles

```{r monthly-tsibble}
# create a monthly data tsibble from cleaned daily tsibble

ts_monthly_total <- ts_daily_total |> 
  index_by(year_month = ~ yearmonth(.)) |> 
  summarise(monthly_total_cyclists = sum(daily_total_cyclists),
            monthly_total_violations = sum(daily_total_violations, na.rm = TRUE)) |> 
  rename(violation_date = year_month)


ts_yearly_total <- ts_daily_total |> 
  index_by(year = ~ year(.)) |> 
  summarise(yearly_total_cyclists = sum(daily_total_cyclists),
            yearly_total_violations = sum(daily_total_violations, na.rm = TRUE)) |> 
  rename(violation_date = year)

```

# EDA: General

## What's the date range for the data?

```{r}
date(range(df_bike_violations$violation_date))
```

## Which dates had the highest rider counts?

```{r table-top-10-busiest-cyclist-days}
#library(finalfit)

df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  arrange(desc(daily_total_cyclists)) |>
  distinct(violation_date, daily_total_cyclists) |>
  slice(1:10) |>
  kable(caption = "Top 10 Busiest Cycling Days", col.names = linebreak(c('Violation Date', 'Daily Total Cyclists'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")

```

Takeaway: Busiest single days were mostly in the autumn of 2019.

## Which dates had the highest number of violations, and how many violations?

```{r table-top-10-busiest-violation-days}
#install.packages('kableExtra')
library(kableExtra)
# Source: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html

df_bike_violations

df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  group_by(violation_date) |> 
  summarise(daily_total_violations = n()) |> 
  arrange(desc(daily_total_violations)) |>
  slice(1:10) |>
  kable(caption = "Top 10 busiest violation days", col.names = linebreak(c('Violation Date', 'Daily Total Violations'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")
```

Takeaway: mostly warmer days in 2018 and 2019, pre-pandemic.

# EDA: Bike Counters

## Total bike counters

```{r, message=FALSE}
df_bicycle_counters_boroughs <- readr::read_csv("Data/Processed/df_bicycle_counters_boroughs.csv")

nrow(df_bicycle_counters_boroughs)
```

## Geoplot: bike counters

```{r geoplot-bike-counters}
mapview(na.omit(df_bicycle_counters_boroughs), xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)
```

## Bike counters per borough

```{r table-bike-counters-per-borough}
df_bicycle_counters_boroughs |> 
  group_by(borough) |> 
  count() |> 
  kable(caption = 'Bike Counters per borough', col.names = linebreak(c('Borough', 'Number'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")
```

Takeaway: mostly in Manhattan. Even the ones in the Bronx and SI are located near Manhattan access points.

## Bike counters used per year

```{r, rows.print = 12}
df_counts |> 
  mutate(id = as.factor(id)) |> 
  group_by(year=year(date)) |> 
  distinct(id) |> 
  summarise(n=n()) 
```

Takeway: It looks like max per year is 14. so not all are 18 used at once.

## Which bike counters counted the most cyclists in 2022?

```{r, message=FALSE}
df_counts |> 
  mutate(date = date(date)) |>
  filter(year(date)==2022) |> 
  group_by(id, year(date)) |> 
  summarise(sum = sum(counts)) |> 
  arrange(desc(sum))


# geoplot
#df_bike_violations_bicycle_counters |> filter(id==300020904) |> mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)  
```

Takeaway:\
Top 5:\
1. 100009427 with 1964902: Williamsburg side of Williamsburg bridge.\
2. 100009428 with 1818163: Queens side of Queensboro Bridge\
3. 100047029 with 1584788: Manhattan side of Manhattan bridge\
4. 100010019 with 1066870: Kent ave in Wburg\
5. 300020904 with 1002070: Brooklyn Bridge

## Which bike counters counted the most cyclists in 2023, so far?

```{r}
df_counts |> 
  mutate(date=year(date)) |> 
  filter(date=='2023') |> 
  mutate(id=as.factor(id)) |> 
  summarise(sum=sum(counts), .by=id) |> 
  arrange(desc(sum))
```

Takeaway: Top 3 are the same. 4 and 5 are switched

```{r top-three-counters-2022-2023-names}
# get names of top 3 counters
df_bicycle_counters_boroughs |> 
  filter(id %in% c(100009427, 100009428, 100047029))
```

## Time series plot for Wburg bridge bike path bike counter

Slope of wburg daily ridership line since 2018

```{r message=FALSE, warning=FALSE}
df_counts |> 
  filter(id==100009427) |> 
  mutate(date = date(date)) |> 
  filter(year(date) >= 2018) |> 
  group_map(~ broom::tidy(lm(daily_total ~ date, data = .x))) 

df_counts_wburg_bridge <-df_counts |> filter(id==100009427)

mod <- lm(daily_total ~ date(date), data=df_counts_wburg_bridge)
  
mod
```

Takeaway: 4.9 riders per day increase for wburg bridge since 2018

Plot:

```{r Plot-wburg-bridge-daily-counts, message=FALSE}
df_counts |> 
  filter(id==100009427) |> 
  mutate(date = date(date)) |> 
  filter(year(date) >= 2018) |> 
  distinct(date, .keep_all = TRUE) |> 
  ggplot(aes(x=date, y=daily_total))+
  geom_line()+
  geom_smooth(method = lm)+
  #geom_abline(slope = coef(lm(daily_total ~  date(date), data=df_counts))[2], intercept = coef(lm(daily_total ~  date(date), data=df_counts))[1])+
  labs(title="Williamsburg bridge bike path daily counts since 2018")
```

Takeaways: \* Seasonality trend - fewer riders in the colder months, more in the summer months. (Note that when graphing all time points since 2014, there was a big increase in the winter of 2016. This was due to the new citibikes, as well as an unusually warm winter). \* Ridership growth since 2018 is \~6.5 increase per day. \* Big bump in the fall of 2019. Bump in the summer after covid lockdown, Slight dip in the summr of 2021, but rising again in 2022.


#### test: slopes for summer and winter

```{r}
# summer (~W18 to ~W35): 

# but just July was easier.  weeks 27-31
df_counts_wburg_bridge_summer <- df_counts |> 
  filter(id==100009427) |> 
  filter(year(date) >= 2018) |> 
  mutate(date = yearweek(date)) |> 
  filter(week(date) == 27:31)

df_counts_wburg_bridge_summer

mod_summer <- lm(weekly_total ~ date, data=df_counts_wburg_bridge_summer)
  
mod_summer


# slope is 42.71 

# winter:

# Just Jan was easiest, weeks 1-5
df_counts_wburg_bridge_winter <- df_counts |> 
  filter(id==100009427) |> 
  filter(year(date) >= 2018) |> 
  mutate(date = yearweek(date)) |> 
  filter(week(date) == 1:5)

df_counts_wburg_bridge_winter

mod_winter <- lm(weekly_total ~ date, data=df_counts_wburg_bridge_winter)
  
mod_winter
summary(mod_winter)

# slope: 36.87

```
So Summer is increasing on average at a faster rate than winter, since 2018.  

attempt to plot winter and summer slopes

```{r message=FALSE, warning=FALSE}
df_counts |> 
  filter(id==100009427) |> 
  mutate(date = date(date)) |> 
  filter(year(date) >= 2018) |> 
  distinct(date, .keep_all = TRUE) |> 
  ggplot(aes(x=date, y=weekly_total))+
  geom_line()+
  geom_abline(slope=mod_winter[1]$coefficients[2], intercept = mod_winter[1]$coefficients[1])+
    geom_abline(slope=mod_summer[1]$coefficients[2], intercept = mod_summer[1]$coefficients[1])+
  geom_smooth(method = lm)+
  #geom_abline(slope = coef(lm(daily_total ~  date(date), data=df_counts))[2], intercept = coef(lm(daily_total ~  date(date), data=df_counts))[1])+
  labs(title="Williamsburg bridge bike path weekly counts since 2018")


```



# EDA: Violation Codes

## How many unique violations were handed out to cyclist?

```{r}
length(fct_unique(df_bike_violations$violation_code)) # 188
```

## How many total violations were handed out to cyclists since 2018?

```{r}
length(df_bike_violations$violation_code)  # 126701
```

## What were the most common violations?

```{r table-10-most-common-violations-table, message=FALSE}
df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  summarise(
    total = sum(n()),
    .by = c(violation_code, description)
  ) |>
  arrange(desc(total)) |>
  mutate(percent = 100 * round(total / sum(total), 3)) |>
  slice(1:10) |>
    kable(caption = 'Top 10 Violations', col.names = linebreak(c('Violation Code', 'Description', 'Total', 'Percent'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")

# note:  12332: ATTACHING SELF TO MOVING MOTOR VEHICLE a/k/a, the Marty McFly rule
```

Takeaway: 44.1% + 5.3% ≈ 50% were for failing to stop at a red light.

### Unique violation: code 12332: Marty McFly

```{r Marty-McFly}
df_bike_violations |> 
  filter(violation_code==12332) |> 
  select(violation_code, violation_date, description)

df_bike_violations |> 
  filter(violation_code==12332) |> 
  mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)
```



## Did the type of violations change after covid?

Let's look at top 10 before lockdown and top 10 after.

Before: 

```{r}

df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  filter(violation_date < '2020-04-01') |> 
  summarise(
    total = sum(n()),
    .by = c(violation_code, description)
  ) |>
  arrange(desc(total)) |>
  mutate(percent = 100 * round(total / sum(total), 3)) |>
  slice(1:10) |>
    kable(caption = 'Top 10 Violations, pre-Covid lockdown', col.names = linebreak(c('Violation Code', 'Description', 'Total', 'Percent'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")
```

After

```{r}
df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  filter(violation_date > '2020-04-01') |> 
  summarise(
    total = sum(n()),
    .by = c(violation_code, description)
  ) |>
  arrange(desc(total)) |>
  mutate(percent = 100 * round(total / sum(total), 3)) |>
  slice(1:10) |>
    kable(caption = 'Top 10 Violations, post-Covid lockdown', col.names = linebreak(c('Violation Code', 'Description', 'Total', 'Percent'), align = "l")) |> 
  kable_classic(full_width = F, html_font = "Cambria", position = "left")
```

Takeaway:  Post-covid lockdown, you are more likely to get a ticket by disobeying a red light.



## Plot: Total violations per year, per borough.

```{r plot-Yearly-Cyclist-Violations-per-borough}
df_bike_violations |> 
  group_by(year=year(violation_date), city_nm) |> 
  ggplot(aes(x=year, fill=city_nm))+
  geom_bar()+
  #geom_bar(position = position_dodge())+
  labs(title="Yearly Cyclist Violations", fill="Borough")+
  scale_fill_brewer()
```

Takeaway: Large drop across the board in violations after 2019, especially in Manhattan. Barely any in Staten Island

## Geocode: Violations location clusters

```{r}
# eomap of violations, post-covid
# wasnt able to use dplyr due to missing values.
df_fct_lump_post_covid <- df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2020-04-01")


mapview(na.omit(df_fct_lump_post_covid), xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.1)
```

map is too full, let's just view since April 2022

```{r}
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-04-01") |>
  mapview(xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.2)
```

better, but still not too clear.

How about just one month - May 2022?

```{r geoplot-vilations-may-2022}
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-05-01" & date <= "2022-05-31") |> 
  mapview(xcol = "longitude", ycol = "latitude", zcol = "fct_lump", crs = 4269, grid = FALSE, alpha.regions = 0.2)
```

What were the total number of violations in this time frame, May of 2022?

```{r}
df_bike_violations |> 
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-05-01" & date <= "2022-05-31") |>   
  nrow()
```

For the sake of clustering, let's just view the most common violation, 11111D1C, for the previous year of data (since June of 2022):

```{r geomap-most-common-violation-past-year}
# Let's just view fct_lump==1111D1C for the previous year:
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-06-01") |>
  filter(fct_lump == "1111D1C") |>
  mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE, alpha.regions = 0.2)

```

What were the total number of 1111D1C violations in this time frame?

```{r}
df_bike_violations |> 
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2022-06-01") |>
  filter(fct_lump == "1111D1C") |>
  nrow()
```

Takeaway: We have a better view of the major ticketing areas for 1111D1C: \* UWS \* UES (and their corresponding avenues up through Harlem), \* Up and down 1st and 2nd aves, especially in the east village, \* On the Brooklyn side of the Williamsburg Bridge, \* Up and down 4th and 5th aves in Brooklyn, \* Liberty Ave in Ozone park/ Richmond Hill, \* The East side of Prospect Park on Bedford Ave \* Bensonhurst, bk.

## Is there a trend for 1111D1C over time?

```{r}
df_bike_violations |> 
  mutate(date = date(violation_date)) |>
  filter(violation_code=="1111D1C") |> 
  group_by(date) |> 
  count() |> 
  ggplot(aes(x=date, y=n))+
  geom_line()
```

Takeaway: the trend does not look out of the ordinary.



# EDA: Daily Cyclists

## Plot: daily total cyclists over time

```{r plot-total-daily-cyclists-over-time, message=FALSE }
ts_daily_total |>
  autoplot(daily_total_cyclists) +
  geom_smooth(method = "lm")
```

Takeaway: ridership did not change much pre vs. post pandemic. not even during lockdown, although possible hiccup. However, ridership seems to be increasing in the colder months while remaining level in the warmer months (This was shown to be false when analyzing slope). Since the ridership is clearly cyclical, an average won't tell us as much as we like. But we can still see an increase over time. However, this could be influence by less ridership earlier on (in 2018).  

### Slope: Daily increase in number of cyclists

```{r message=FALSE }
coef(lm(ts_daily_total$daily_total_cyclists~ts_daily_total$violation_date))[2]

```

## Plot: yearly trend in number of cyclists, to 2022

```{r message=FALSE }
plot1<-df_counts |> 
  #mutate(date = mdy_hms(date)) |> 
  group_by(date=year(date)) |> 
  summarise(yearly = sum(counts)) |> 
  filter(date < 2023) |> 
  ggplot(aes(x=date, y=yearly))+
  geom_line()+
  scale_x_continuous(breaks=seq(2012, 2022,2))+
  labs(title='Yearly total cyclists', y='Cyclists')


plot2 <-df_counts |> 
  mutate(id = as.factor(id)) |> 
  group_by(year=year(date)) |> 
  distinct(id) |> 
  summarise(n=n()) |> 
  ggplot(aes(x=year, y=n))+
  geom_line()+
  scale_x_continuous(breaks=seq(2012, 2022,2))+
  labs(title='Bike Counters per Year', y='Bike Counters')

plot1 + plot2
```

This isn't too accurate early on, as there were fewer bike counters used.

## Histogram: daily total cyclists

```{r hist-daily-total-cyclists}
ts_daily_total |>
  ggplot(aes(x = daily_total_cyclists)) +
  geom_histogram(bins = 80)
```

## Does ridership increase or decrease yearly?

```{r plot-yearly-ridership}
# leave out 2023 since we only have up to March

ts_yearly_total |> 
  filter_index(. ~ "2022") |>
  autoplot(yearly_total_cyclists)
```

Takeaway: Yes, it increases. but at a slower rate after covid

## Does ridership change throughout the year (seasonality)?

```{r}
# yearly seasonal plot
ts_daily_total |>
  gg_season(daily_total_cyclists)
```

Takeaway - it's roughly the same yearly cycle per year. increase in ridership from May-Oct, decrease Nov-Apr

A plot with monthly totals would look cleaner while still conveying enough information

```{r plot-seasonal-monthly-totals}
ts_monthly_total |>
  gg_season(monthly_total_cyclists)
```

Takeaway - dip in April 2020 during covid lockdown, but not much less than before.

```{r yearly-ridership-trends-by-month}
ts_monthly_total |>
  gg_subseries(monthly_total_cyclists)
```

Takeaways: \* Seasonality is clearer. Monthly differences are clearer. \* 2023 is turning out to be noticeably more riders than previous years in colder months of Jan and Feb.

Ridership changes over time takeaways: \* Ridership is increasing. \* Clear yearly seasonality, with May-Oct being the busiest, and Nov-Apr being less busy.

## Daily total cyclists decomposition

### Classical Decomp

```{r classical-decomp, message=FALSE, warning=FALSE }
ts_daily_total |>
  #  filter(!is.na(daily_total_cyclists)) |>
  #  fill_gaps() |>
  model(classical_decomposition(daily_total_cyclists, type = "additive")) |>
  components() |>
  autoplot()
```

Takeaway: Classical decomposition does not handle multi-seasonality of the data too well.

### STL Decomp

```{r stl-decomp, message=FALSE, warning=FALSE }
ts_daily_total |>
  model(
    STL(daily_total_cyclists ~ trend(window = 365) +
      season(period = "year") + season(period = "1 month"), )
  ) |>
  components() |>
  autoplot()
```

Takeway: STL is best

### Moving average

```{r moving-average, message=FALSE, warning=FALSE }
# what about moving average for trend?  it ok.  I think stl is best.
ts_daily_total |>
  mutate(`5-MA` = slider::slide_dbl(daily_total_cyclists, mean, .before = 90, .after = 90, .complete = TRUE)) |>
  autoplot(daily_total_cyclists) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00")
```

# EDA: Daily Violations

## Plot: daily total violations over time

```{r plot-daily-total-violations}
ts_daily_total |>
  autoplot(daily_total_violations)
```

Takeaway: There were more violations before the pandemic. During lockdown, very few.

## Histogram: daily total violations

```{r message=TRUE, warning=FALSE}
ts_daily_total |>
  ggplot(aes(x = daily_total_violations)) +
  geom_histogram(bins = 80)
```

## Are the yearly/monthly number of violations trending up or down?

```{r plot-yearly-total-violations}
ts_yearly_total |> 
  filter_index(. ~ "2022") |>
  autoplot(yearly_total_violations)+
  ylim(0, NA)
```

Takeaway: large drop in violations after lockdown, but staying steady.

```{r plot-monthly-total-violations}
# monthly:  just cleaner graph of daily violations over time.  Post-covid looks steady.
ts_monthly_total |>
  autoplot(monthly_total_violations)
```

Takeaway: Some seasonality - more violations in warmer months, dips to fewer in colder. This seasonality was less pronounced during 2020, but seems to be going back to previous trends in 2022

## Do the number of violations change throughout the year (seasonal)?

Let's look at just post-lockdown:

```{r plot-viol-seasonality-post-lockdown}
ts_daily_total |>
  filter_index("2020-03" ~ .) |>
  gg_season(daily_total_violations)
```

Monthly is easier to see

```{r plot-viol-seasonality-monthly-post-lockdown}
ts_monthly_total |>
  filter_index("2020-03" ~ .) |>
  gg_season(monthly_total_violations)
```

Let's view a subseries plot

```{r plot-viol-seasonality-subseries-post-lockdown}
ts_monthly_total |>
  filter_index("2020-03" ~ .) |>
  gg_subseries(monthly_total_violations)
```

Takeawy: Hard to see any trends, but fewer violations in Nov/Dec/Jan/Feb. Most in Sept, March, August. April has a lower average due to covid lockdown in 2020.

## As the number of riders increases, does the number of violations also increase?

```{r message=FALSE}
# utilizes corrr library

df_bike_violations |>
  mutate(violation_date = date(violation_date)) |>
  group_by(violation_date) |>
  mutate(daily_total_violations = sum(n())) |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() 
```

Takeway: cor: 0.05554951 So no, as the number of riders increases, the number of violations does not.

## However, Does this differ for pre-covid and post-lockdown?

### Plot: daily violations vs. daily cylists, since 2018

```{r}
ts_daily_total |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point()
```

### Plot: daily violations vs. daily cylists, Pre-lockdown

```{r message = FALSE, warning=FALSE}
# correlation:  0.46
cor <- ts_daily_total |>
  filter_index(~"2020-04") |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() |>
  slice(1) |>
  select(daily_total_violations) |>
  as.numeric()

# pre-covid
ts_daily_total |>
  filter_index(~"2020-04") |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point() +
  geom_smooth(method = lm) +
  sm_statCorr(label_x = 250, label_y = 1000)
```

### Plot: daily violations vs. daily cylists, Pose-lockdown

```{r message=FALSE}
# devtools::install_github('smin95/smplot2', force = TRUE)
# library(smplot2)

# post-covid
ts_daily_total |>
  filter_index("2020-04" ~ .) |>
  ggplot(aes(x = daily_total_violations, y = daily_total_cyclists)) +
  geom_point() +
  geom_smooth(method = lm) +
  sm_statCorr(label_x = 120, label_y = 1000)

# correlation:  ~0.37
ts_daily_total |>
  filter_index("2020-04" ~ .) |>
  select(daily_total_cyclists, daily_total_violations) |>
  correlate() |>
  kable()

# ans: they differ a little more than all together.  pre-covid was a better indicator.
```

Takeaway: Yes, there is a positive correlation between cyclists per day and violations per day, when the data is split pre- and post-lockdown. It's strength is medium though, and does not tell the whole picture.

## Do largest violations (summed per day) change over time?

```{r plot-daily-1111D1C, message = FALSE}
# plot: daily biggest violation (1111D1C) over time, post covid
df_bike_violations |>
  mutate(fct_lump = fct_lump(f = df_bike_violations$violation_code, n = 9)) |>
  filter(fct_lump == "1111D1C") |>
  mutate(date = date(violation_date)) |>
  filter(date >= "2020-04-01") |>
  summarise(
    sum = sum(n()),
    .by = c(date, fct_lump)
  ) |>
  ggplot(aes(x = date, y = sum)) +
  geom_line()
```

Takeaway: No. not much different than general daily violations plot. lower amount around Dec/Jan, otherwise fairly steady.


# EDA: Ratio of violations to cyclists

Since the data is seasonal, does the ratio of violations to cyclists even that out?

```{r}
ts_daily_total |> 
  mutate(violations_to_cyclists = daily_total_violations/daily_total_cyclists) |> 
  autoplot(violations_to_cyclists)
```

Takeaway: Possibly, but it doesn't look like it. The ratio changed after covid lockdown.

Let's just look at post-covid:

```{r}
ts_daily_total |> 
  mutate(violations_to_cyclists = daily_total_violations/daily_total_cyclists) |> 
  filter_index("2020-04"~.) |> 
  autoplot(violations_to_cyclists)
```

## Can we view the trend of the ratio?

Let's first look at moving average: 

```{r message=FALSE, warning=FALSE}
ts_daily_total |>
  mutate(daily_total_violations = replace_na(daily_total_violations, 0)) |> 
  mutate(violations_to_cyclists = daily_total_violations/daily_total_cyclists) |> 
  filter_index("2020-04"~.) |> 
  mutate(`5-MA` = slider::slide_dbl(violations_to_cyclists, mean, .before = 30, .after = 30, .complete = TRUE)) |>
  autoplot(violations_to_cyclists) +
  geom_line(aes(y = `5-MA`), colour = "#D55E00")
```


STL Decomposition:

```{r}
ts_daily_total |>
  mutate(daily_total_violations = replace_na(daily_total_violations, 0)) |> 
  mutate(violations_to_cyclists = daily_total_violations/daily_total_cyclists) |> 
  filter_index("2020-04"~.) |> 
  model(
    STL(violations_to_cyclists ~ trend(window = 90) +
      season(period = "year"))
  ) |>
  components() |>
  autoplot()

```

Takeaway: Setting window to 90 looks better.  Takes into account increase in 2023.  Looking at season_year, we see that the ratio peaks in Feb-March or so, then dips, slowly increases until around Nov, then dips the most around New Years. 






```{r scratchpad, include=FALSE}
   

str(df_bike_violations)

# plot shows that ebikes and escooters didnt have their own categories until sometime in mid 2022. before that, everything was under 'bikes'
df_bike_violations |>
  ggplot(aes(x = violation_date, y = city_nm, col = veh_category)) +
  geom_jitter()

# nuthin
df_bike_violations |>
  ggplot(aes(x = violation_date, y = city_nm, col = violation_code)) +
  geom_jitter()

# violations per month
df_bike_violations |>
  group_by(yearmonth = yearmonth(violation_date)) |>
  summarize(total_violations = sum(n())) |>
  ggplot(aes(x = yearmonth, y = total_violations)) +
  geom_line()


# violations per borough per year
df_bike_violations |>
  group_by(year = yearmonth(violation_date), city_nm) |>
  summarize(total_violations = sum(n())) |>
  ggplot(aes(x = year, y = total_violations, col = city_nm)) +
  geom_line()


# cyclist counters per borough:
# library(forcats)
# df_bicycle_counters_boroughs |>
#  ggplot(aes(x = fct_infreq(Borough))) +
#  geom_bar() +
#  labs(x = "Borough", y = "Cyclist Counters")
```

```{r}

```
