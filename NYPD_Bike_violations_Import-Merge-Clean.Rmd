---
title: "R Notebook"
output:
  github_document:
    html_preview: FALSE 
    
---

```{r libraries, message=FALSE}
library(knitr)
library(rmarkdown)
library(fpp3)
library(readxl)
library(httr)
library(glue)
# install.packages('sf')
library(sf)
# install.packages('Rcpp')
library(Rcpp)
# install.packages('mapview')
library(mapview)
# install.packages('tidygeocoder')
library(tidygeocoder)
library(janitor)
library(styler)
```

# Import, Merge, and Clean Data

## Violation Codes data

### Import violations xlsx

```{r}
# import Code Violations xlsx
# After cleaning, his file will contain the violation code numbers and their corresponding descriptions.  

# source:  https://dmv.ny.gov/e-data/violationcodes.xlsx
# library(readxl)
# library(httr)

path <- "https://dmv.ny.gov/e-data/violationcodes.xlsx"

GET(path, write_disk(tf <- tempfile(fileext = ".xlsx")))
df_violation_codes <- read_xlsx(tf, sheet = 1)

df_violation_codes <- df_violation_codes |>
  clean_names() |>
  rename("violation_code" = "adj_code") |>
  select(c("violation_code", "description"))
```

### Clean violations codes

```{r}
# Remove duplicates

# VIOLATION_CODE duplicates found: (1110A and 1151A)

# This step proved to be necessary, as merging the two documents would duplicate the number of rows of any violation code in df to match any duplicates in violations.  This caused the # of rows of df to be over 200,000, when it should have remained the same (184770)

df_violation_codes |>
  count(violation_code) |>
  arrange(desc(n))

# view descriptions for 1110A
df_violation_codes |>
  filter(violation_code == "1110A")
# DISOBEYED TRAFFIC DEVICE
# DISOBEYED TRAFFIC DEVICE - PAVEMENT MARKINGS
# we will change all to the second, longer description

# view descriptions for 1151A
df_violation_codes |>
  filter(violation_code == "1151A")
# FAILED TO YIELD RIGHT-OF-WAY TO PEDESTRIAN IN CROSSWALK
# FAILED TO YIELD RIGHT-OF-WAY TO PEDESTRIAN ON SIDEWALK
# We will change all to read:
# FAILED TO YIELD RIGHT-OF-WAY TO PEDESTRIAN ON SIDEWALK OR CROSSWALK


# Remove duplicates of violation codes
# and manually merge descriptions for duplicate violation_codes

df_violation_codes_no_dupes <- df_violation_codes |>
  distinct(violation_code, .keep_all = TRUE) |>
  mutate(description = case_when(violation_code == "1151A" ~ "FAILED TO YIELD RIGHT-OF-WAY TO PEDESTRIAN ON SIDEWALK OR CROSSWALK", TRUE ~ description)) |>
  mutate(description = case_when(violation_code == "1110A" ~ "DISOBEYED TRAFFIC DEVICE - PAVEMENT MARKINGS", TRUE ~ description))

# OPTIONAL double-check no dupes
# df_violation_codes_no_dupes |>
#   count(violation_code) |>
#   distinct(n)

# OPTIONAL double-check that 1110A and 1151A read what they are supposed to
# df_violation_codes_no_dupes |>
#   filter(violation_code=='1151A'|violation_code=='1110A')

dim(df_violation_codes_no_dupes)
# View(violations_no_dupes)

```

## NYPD Violations data

### Import current violations data

```{r}
# only run once

# import NYPD Summons violations csv

# source: https://data.cityofnewyork.us/Public-Safety/NYPD-B-Summons-Year-to-Date-/57p3-pdcj

df_current_violations <- readr::read_csv("Data/Raw/NYPD_B_Summons__Year_to_Date_.csv")

# Remove unneeded columns.  update column types as needed
# CHG_LAW_CD - can be removed.  It just states whether the violation was NY state or local NYC law
# JURIS_CD - can be removed.  no info in metadata.  only one distinct value.
# VIOLATION_TIME - redundant
# RPT_OWNING_CMD - precinct where reported. change this from integer to factor, since it acts similar to a zip code.

df_current_violations <- df_current_violations |>
  clean_names() |>
  mutate(violation_code = as.factor(violation_code)) |>
  mutate(veh_category = as.factor(veh_category)) |>
  mutate(city_nm = as.factor(city_nm)) |>
  mutate(rpt_owning_cmd = as.factor(rpt_owning_cmd)) |>
  mutate(violation_date = mdy(violation_date) + hms(violation_time)) |>
  select(-c(chg_law_cd, violation_time, juris_cd)) |>
  rename("location" = "location_point")

str(df_current_violations)
```

### Import historic violations data

```{r}
# source: https://data.cityofnewyork.us/Public-Safety/NYPD-B-Summons-Historic-/bme5-7ty4

path <- "https://www.dropbox.com/s/9pikbkygm7ywq2h/NYPD_B_Summons__Historic_.csv?dl=1"

df_historic <- readr::read_csv(path)

df_historic <- df_historic |>
  clean_names() |>
  mutate(violation_code = as.factor(violation_code)) |>
  mutate(veh_category = as.factor(veh_category)) |>
  mutate(city_nm = as.factor(city_nm)) |>
  mutate(rpt_owning_cmd = as.factor(rpt_owning_cmd)) |>
  mutate(violation_date = mdy(violation_date) + hms(violation_time)) |>
  select(-c(chg_law_cd, violation_time, juris_cd))

str(df_historic)
```

### Merge historic violations data with current violations data

```{r}
df_merged <- merge(df_historic, df_current_violations, all.x = TRUE, all.y = TRUE)

# for whatever reason, violation_code was not saved as a factor when merged
df_merged <- df_merged |>
  mutate(violation_code = as.factor(violation_code))

str(df_merged)
# length(unique(df_merged$violation_code))
```

### Merge merged violation data with violation code description data

```{r}
# matches violation description with violation code.  makes it easier to see what the violation code refers to.

df_merged <- df_merged |>
  left_join(x = df_merged, y = df_violation_codes_no_dupes, by = "violation_code")

df_merged <- df_merged |>
  mutate(violation_code = as.factor(violation_code))


# OPTIONAL - double check values for code 1110A
# df |>
#   filter(violation_code=='1110A')

str(df_merged)
tail(df_merged)
```

### Clean merged data

```{r}
# ignore this cell

# attempt to find missing dates

# To Double Check: possibly missing dec 31 2018 and dec 31 2019.

# source: https://stackoverflow.com/questions/50724137/find-missing-days-in-r-date-sequence
date_range <- seq(min(date(df_merged$violation_date)), max(date(df_merged$violation_date)), by = 1)

date_range[!date_range %in% date(df_merged$violation_date)]

length(unique(date(df_merged$violation_date)))

max(date(df_merged$violation_date)) - min(date(df_merged$violation_date))
```

## Count Data

### Import total rider counts

```{r}
# https://data.cityofnewyork.us/Transportation/Bicycle-Counts/uczf-rk3c

path2 <- "https://www.dropbox.com/s/oaklaldm8td9wem/Bicycle_Counts.csv?dl=1"
df_counts_raw <- read.csv(path2)

df_counts <- df_counts_raw |>
  mutate(date = mdy_hms(date))

head(df_counts)
str(df_counts)

df_counts_raw |>
  distinct(id) |>
  arrange(id)
```



### Create daily and weekly rider total columns

```{r}
# create new columns for daily and weekly totals
# Note: This will be updated later, when duplicate counters are removed. However, this is useful in order to show which counters are duplicates.

df_counts <- df_counts |>
  group_by(date(date)) |>
  mutate(daily_total = sum(counts)) |>
  ungroup() |>
  group_by(yearweek(date)) |>
  mutate(weekly_total = sum(counts)) |>
  ungroup() |>
  arrange(date) |>
  select(-c("date(date)", "yearweek(date)"))

head(df_counts)


```

### Create borough df

```{r}
# adds a column that contains each counter's borough.
# source: https://data.cityofnewyork.us/Transportation/Bicycle-Counters/smn3-rzf9


df_bicycle_counters <- read.csv("Data/Raw/Bicycle_Counters.csv")

df_bicycle_counters

# change 0s to na
df_bicycle_counters <- df_bicycle_counters |>
  mutate_at(c("longitude", "latitude"), ~ na_if(., 0))

df_bicycle_counters
# map the stations
# Source: https://map-rfun.library.duke.edu/01_georeference.html

# install.packages('sf')
# library(sf)
# install.packages('Rcpp')
# library(Rcpp)
# install.packages('mapview')
# library(mapview)

# mapview(na.omit(df_bicycle_counters), xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)
```

### Add column with boroughs labels

```{r bike-counts-borough-table}
df_bicycle_counters

# reverse look up by latitude and longitude
# source: https://cran.r-project.org/web/packages/tidygeocoder/readme/README.html

# sometimes this doesn't work.  When it does work, not all are filled in.

# install.packages('tidygeocoder')
# library(tidygeocoder)

df_bicycle_counters_boroughs <- df_bicycle_counters |>
  reverse_geocode(
    lat = latitude, long = longitude, method = "osm",
    address = address_found, full_results = TRUE
  ) |>
  select(id, name, latitude, longitude, suburb) |>
  rename("borough" = "suburb") |>
  mutate(borough = gsub("Richmond County", "Staten Island", borough)) |>
  mutate(borough = gsub("Queens County", "Queens", borough))

# replace willis ave borough na's with 'Bronx', and give manhattan bridge interference corresponding borough name
df_bicycle_counters_boroughs$borough[grepl("Willis", df_bicycle_counters_boroughs$name)] <- "Bronx"
df_bicycle_counters_boroughs$borough[grepl("Manhattan Bridge Interference", df_bicycle_counters_boroughs$name)] <- "Manhattan"



df_bicycle_counters_boroughs |>
  filter(!is.na(latitude)) |>
  tabyl(borough) |>
  adorn_pct_formatting() |>
  adorn_totals()
```

### Remove duplicate bike counters

```{r}

# first, find counters that share a latitude value
df_bicycle_counters_boroughs |> 
  mutate(lat_sum = sum(n()), .by=latitude) |> 
  ungroup() |> 
  arrange(desc(lat_sum), latitude) 

# observations: 

## 1. willis ave has 3 
# according to metadata file (DataDictionary-BicycleCounters.xlsx on web page with original dataset), the willis ave station counts bikes (300029647), pedestrians (300029648) and has the sume of those (300028963).  Therefore, let's remove the two that are not exclusively bikes (removed below).

## 2. amsterdam/columbus has 2 at the same lat/lon.  should one be deleted?  the streets are a block away and run in different directions. Check to see if totals are equal.
# test to see if counts for a single day are the same:
df_counts_raw |> 
  filter(id==100057320 | id==100057319) |> 
  mutate(date=mdy_hms(date)) |> 
  mutate(date = date(date)) |> 
  filter(date== '2020-04-14') |> 
  group_by(id) |> 
  summarize(sum = sum(counts))
# the counts are different, so we can leave both in.

## 3.  manhattan bridge has 2+2+2 for 'interference' + forsythe plaza.
# let's look at all man bridge.
man_br_ids <- df_bicycle_counters_boroughs |> 
  filter(grepl(paste(c("Manhattan Bridge", "Forsyth"), collapse='|'), name)) |> 
  pull(id)

man_br_ids

# view all man bridge (un-hash for map)
df_bicycle_counters |> 
  filter(id %in% man_br_ids) # |> filter(!is.na(longitude)) |> mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)

#view count sums, check for max and doubles
df_counts_raw |> 
  filter(id %in% man_br_ids) |> 
  group_by(id) |> 
  summarize(sum = sum(counts))
# from this we can see that 100047029 and 100062893 are the biggest counters with 14012230 each. 

df_bicycle_counters_boroughs |> 
  filter(id==100047029|id==100062893)
# 'Manhattan Bridge Display Bike Counter'	and 'Manhattan Bridge Bike Comprehensive'

# double-check date range (min and max)
df_counts_raw |> 
  mutate(date = mdy_hms(date)) |> 
  filter(id==100039064) |> 
  slice(which.min(date),which.max(date))
# From metadata file:  'As of May 7, 2019 the Display Manhattan Side counter is the primary source of bike counts on the bike path of the bridge'. since the range is 2012-2023, let's choose just the Manhattan Bridge Display Bike Couinter 100047029 for all man bridge data.  Removed below.



# note there are some brooklyn bridge dupes as well, in 2023 data:
df_counts_raw |> 
  mutate(date = mdy_hms(date)) |> 
  mutate(date=year(date)) |> 
  #filter(date=='2023') |> 
  mutate(id=as.factor(id)) |> 
  filter(id %in% c(100010022, 300020241, 300020904)) |> 
  summarize(sum=sum(counts), .by=id) |> 
  arrange(sum)


# since 300030904 'Comprehensive Brooklyn Bridge Counter' has more total than the others, let's keep that one and discard 300020241, 100010022.  it's date range is also longer than the other two.
  
df_bicycle_counters_boroughs |> 
  filter(grepl('Brooklyn', name))



# filter out all duplicate/unnecessary counters (includes 'Test' 300020692):
df_bicycle_counters_boroughs<-df_bicycle_counters_boroughs |> 
  filter(!id==300028963 & !id==300029648 &!id==100009426 & !id==100055175 & !id==100009429 & !id==100005020 & !id==100051865 & !id==100048744 & !id==100062893 & !id==300020692 & !id==300020241 & !id==100010022 & !id==100039064)

# how many counters now? should be 18
dim(df_bicycle_counters_boroughs)


# Optional:  Save to csv
# path_name<-'df_bicycle_counters_boroughs.csv'
# write.csv(df_bicycle_counters_boroughs, path_name, row.names=FALSE)
# df_bicycle_counters_boroughs

```


### Update df_counts by removing duplicate counters
```{r}

# 29 distinct counter id's.  need to be reduced
df_counts |> 
  distinct(id)

bike_counter_ids_no_dupes<-df_bicycle_counters_boroughs |> 
  pull(id)


df_counts <- df_counts |> 
  filter(id %in% bike_counter_ids_no_dupes) |> 
  #distinct(id) |> 
  group_by(date(date)) |>
  mutate(daily_total = sum(counts)) |>
  ungroup() |>
  group_by(yearweek(date)) |>
  mutate(weekly_total = sum(counts)) |>
  ungroup() |>
  arrange(date) |>
  select(-c("date(date)", "yearweek(date)"))

# double-check distinct bike counter id's.  Should be 18
df_counts |> 
  distinct(id)


# optional: save csv to desktop, to upload to dropbox.  It is too large for github
#write.csv(df_counts, '/Users/<username>/Desktop/df_counts.csv', row.names=FALSE)

```



### Incomplete - Merge borough data with count data

```{r}
# Ignore cell - I'm doubtful that this step is necessary, since the counter locations are so heavily skewed for Manhattan


sort(unique(df_counts_raw$id)) == sort(unique(df_bicycle_counters_boroughs$id))

# False:
length(unique(df_bicycle_counters_boroughs$id)) == length(unique(df_counts_raw$id))

sort(unique(df_counts_raw$id))
# 100055175 is not in both

df_counts_raw |>
  filter(id == 100055175)


sort(unique(df_bicycle_counters_boroughs$id))
# 300020692 is not in both

# its just a 'test' and can be removed.
df_bicycle_counters_boroughs |>
  filter(id == 300020692)

# left off - trying to find out which id's match, in order to merge on id.
```

## Merge violations data with rider count data

```{r}
# create 'date' column of just ymd (no hms) in df_counts to merge on
df_counts <- df_counts |>
  mutate(date = date(date)) |>
  select(-c(countid, id, status))

# match date column with equal one in df_merged.  Just choose bikes, as the count data is just for bikes.  Also makes merging go quicker.
df_bike_violations <- df_merged |>
  filter(veh_category %in% c("BIKE", "EBIKE", "ESCOOTER")) |>
  mutate(date = date(violation_date)) |>
  left_join(y = df_counts, by = "date", multiple = "first") |>
  select(-c("counts", "date")) |>
  rename("daily_total_cyclists" = "daily_total") |>
  rename("weekly_total_cyclists" = "weekly_total") |>
  droplevels() |> 
  arrange(violation_date)
```

### Check for NAs

```{r}
# check for na's

df_bike_violations |> 
  summarise(across(everything(), ~ sum(is.na(.x)))) |> 
  t()

# there are 103 'na' entries for location coordinate data.  We can ignore these for now, and remove them when data location is required.  There is no sense in discarding the rest of the information, like violation codes, which can be used.

```


#### Clean na values in description

```{r}
# check to see which Violation codes correspond to empty description cells

df_bike_violations |>
  filter(is.na(description)) |>
  distinct(violation_code)

# There were 4 distinct Violation codes with no description:
# 37524A (NOTE: after adjusting this value, this is the 5th most common violation code) - since 37524 is UNLAWFUL USE OF TV RECEIVING SET and 37524AB is OPER BICYCLE WITH MORE 1 EARPHONE, I will take the liberty to change all of these to 37524AB.
# 4014 does not exist, but 4014B is COMMERCIAL VEHICLE ON PARKWAY- NYC.  Change all to 4014B
# 4021 is associated with 4 different violation codes:  4021A - NO LICENSE PLATE OR SINGLE LICENSE PLATE, 4021B1 - DIRTY PLATE OR PLATE COVERED BY GLASS OR PLASTIC, 4021B2 - PLATE KNOWINGLY COVERED WITH INTENT TO OBSCURE, 4021B3 - PLATE OBSCURED BY VEHICLE OR ANYTHING CARRIED THERON.  Since these are all related to license plates, I will change these all to 4021A
# 22653 - this was closest to two values, 22651 - EQUIPMENT VIOLATION LIMITED USE VEHICLE and 22651M - MISCELLANEOUS EQUIPMENT VIOLATION - LIMITED USE MOTORCYCLE.  Since all 18 of the 22653 violations were either e-bike or escooter, I will change all of these to 22651M

# df_bike_violations |>
#  filter(violation_code=='22653')


# change all violation codes to decided on values
df_bike_violations <- df_bike_violations |>
  mutate(violation_code = replace(violation_code, violation_code == "37524A", "37524AB")) |>
  mutate(violation_code = replace(violation_code, violation_code == "4014", "4014B")) |>
  mutate(violation_code = replace(violation_code, violation_code == "4021", "4021A")) |>
  mutate(violation_code = replace(violation_code, violation_code == "22653", "22651M"))

# change all descriptions baseed on new violation code values
df_bike_violations$description[df_bike_violations$violation_code == "37524AB"] <- "OPER BICYCLE WITH MORE 1 EARPHONE"
df_bike_violations$description[df_bike_violations$violation_code == "4014B"] <- "COMMERCIAL VEHICLE ON PARKWAY- NYC"
df_bike_violations$description[df_bike_violations$violation_code == "4021A"] <- "NO LICENSE PLATE OR SINGLE LICENSE PLATE"
df_bike_violations$description[df_bike_violations$violation_code == "22651M"] <- "MISCELLANEOUS EQUIPMENT VIOLATION - LIMITED USE MOTORCYCLE"

# double-check that there are no violatinn codes with na's in the description
df_bike_violations |>
  filter(is.na(description)) |>
  distinct(violation_code)
```


#### Clean city_nm NAs

```{r}
# city_nm has empty values that don't say NA.  Let's make them say NA
df_bike_violations$city_nm <- as.character(df_bike_violations$city_nm)
df_bike_violations$city_nm[df_bike_violations$city_nm == ""] <- NA
df_bike_violations$city_nm <- as.factor(df_bike_violations$city_nm)

# Can try to check this for cleaner way to change to NAs in dplyr:  https://stackoverflow.com/questions/24172111/change-the-blank-cells-to-na

# how many city_nm na values now?  #ans: 92
df_bike_violations |>
  group_by(city_nm) |>
  summarize(sum = sum(n()))

# View the 92 na's
df_bike_violations |>
  filter(is.na(city_nm))

# Takeaway: none of the 92 have longitude or latitiude info either.  To be removed in next cell.
```

### Clean: city_nm == 'New York' values changed to their boroughs.

```{r}
head(df_bike_violations)

df_bike_violations |>
  filter(city_nm == "NEW YORK") |>
  # three rows were missing latitude/longitude values.  mapview needs no na's.
  filter(!is.na(latitude) | !is.na(longitude)) |>
  mapview(xcol = "longitude", ycol = "latitude", crs = 4269, grid = FALSE)

# takeaway from graph:  The large majority are in Manhattan, but there are still a few in Queens/Brooklyn

# Can we use reverse geo lookup to get the boroughs?
df_bike_violations |>
  filter(city_nm == "NEW YORK") |>
  # three rows were missing latitude/longitude values.  mapview needs no na's.
  filter(!is.na(latitude) | !is.na(longitude)) |>
  reverse_geocode(
    lat = latitude, long = longitude, method = "osm",
    address = address_found, full_results = TRUE
  )
# select(id, name, latitude, longitude, suburb)
# rename('borough' = 'suburb') |>
# mutate(borough = gsub('Richmond County', 'Staten Island', borough)) |>
# mutate(borough = gsub('Queens County', 'Queens', borough))

# create temp file for merging
df_tmp <- df_bike_violations |>
  filter(city_nm == "NEW YORK") |>
  filter(!is.na(latitude) | !is.na(longitude)) |>
  # slice(1:100) |>
  reverse_geocode(
    lat = latitude, long = longitude, method = "osm",
    address = address_found, full_results = TRUE
  ) |>
  select("evnt_key", "suburb")


df_bike_violations |>
  slice(1:100)

df_bike_violations <- df_bike_violations |>
  left_join(df_tmp, by = "evnt_key") |>
  mutate(city_nm = case_when((city_nm == "NEW YORK" & !is.na(suburb)) ~ suburb, .default = city_nm)) |>
  mutate(city_nm = case_when(city_nm == "Manhattan" ~ "MANHATTAN", .default = city_nm)) |>
  mutate(city_nm = case_when(city_nm == "Kings County" ~ "BROOKLYN", .default = city_nm)) |>
  mutate(city_nm = case_when(city_nm == "Queens County" ~ "QUEENS", .default = city_nm)) |>
  mutate(city_nm = case_when(city_nm == "Brooklyn" ~ "BROOKLYN", .default = city_nm)) |>
  mutate(city_nm = case_when(city_nm == "Queens" ~ "QUEENS", .default = city_nm))

# clean this up if re-running df_bike_violations from scratch
df_bike_violations <- df_bike_violations |>
  select(-suburb)

df_bike_violations |>
  group_by(city_nm) |>
  summarize(sum = sum(n()))
# takeaway: this is better: now only 3 NEW YORK and 92 na.

df_bike_violations |>
  filter(city_nm == "NEW YORK")
# takeaway:  these dont have longitudinal or latitudinal info.  

# Since there are only 95 na, we can drop them all.

df_bike_violations <- df_bike_violations |> 
  filter(!city_nm == "NEW YORK") |> 
  filter(!is.na(city_nm))

df_bike_violations |> 
  str()

head(df_bike_violations)

# are we done with nas?
df_bike_violations |> 
  summarise(across(everything(), ~ sum(is.na(.x)))) |> 
  t()

dim(df_bike_violations)
```

## Optional - save csv

```{r}
# optional: write to csv
# file is approx 25mb. That exceeds limit for uploading via browser, but it is able to upload to git via push
# install.packages("glue")
todays_date <- Sys.Date()
path_name<-glue('Data/Processed/df_bike_violations_{todays_date}.csv')
write.csv(df_bike_violations, path_name, row.names=FALSE)
```
