---
title: "R Notebook"
output:
  github_document:
    html_preview: FALSE 
---


```{r libraries, message=FALSE, warning=FALSE, include=FALSE}
library(knitr)
library(tidyverse)
library(fpp3)
```

# Import daily and monthly tsibbles

```{r}
# import bike violations

df_bike_violations <- readr::read_csv("Data/Processed/df_bike_violations_2023-08-05.csv")

df_bike_violations <- df_bike_violations |>
  mutate(violation_code = as.factor(violation_code)) |>
  mutate(veh_category = as.factor(veh_category)) |>
  mutate(city_nm = as.factor(city_nm)) |>
  mutate(rpt_owning_cmd = as.factor(rpt_owning_cmd)) |>
  mutate(violation_date = as.POSIXct(violation_date))

# creat daily tsibble
ts_daily_total <- df_bike_violations |>
  mutate(violation_date = as_date(violation_date)) |>
  group_by(violation_date, daily_total_cyclists) |>
  summarize(daily_total_violations = sum(n())) |>
  ungroup() |>
  as_tsibble(index = violation_date)

# fill date gaps
# import df_counts
path3 <- "https://www.dropbox.com/s/nffkpwuz1u6kd5t/df_counts.csv?dl=1"
df_counts <- readr::read_csv(path3)

# fill date gaps for ts_daily_total and join with df_counts
ts_daily_total <- ts_daily_total |>
  fill_gaps() |>
  select(-daily_total_cyclists) |>
  mutate(date = violation_date) |>
  left_join(y = df_counts, by = "date", multiple = "any") |>
  rename(daily_total_cyclists = daily_total) |>
  select(-c("date", "counts", "weekly_total")) |>
  mutate_at("daily_total_violations", ~ replace_na(., 0))

# create a monthly data tsibble from cleaned daily tsibble
ts_monthly_total <- ts_daily_total |>
  index_by(yearmonth(violation_date)) |>
  summarize(across(c("daily_total_violations", "daily_total_cyclists"), ~ sum(.x, na.rm = TRUE))) |>
  rename(monthly_total_cyclists = daily_total_cyclists) |>
  rename(monthly_total_violations = daily_total_violations) |>
  rename(date = "yearmonth(violation_date)")

```


# Forecasting: Monthly total riders

Let's see how the daily totals look

```{r}
ts_daily_total |> 
  autoplot(daily_total_cyclists)
```

Pretty messy.  The monthly totals look easier to read, so let's use those:

```{r}
ts_monthly_total |> 
  autoplot(monthly_total_cyclists)
```


## Train and test sets for monthly data

Training set is through the end of 2021.  Test set is from Jan 2022 until March 2023.

```{r}
train<-ts_monthly_total |> 
  select(!monthly_total_violations) |> 
  filter_index(~"2021-12")

test<-ts_monthly_total |> 
  select(!monthly_total_violations) |> 
  filter_index("2022-01"~.)
```

## Seasonal Naive forecast on monthly totals

```{r}
ts_monthly_snaive_mod <- ts_monthly_total |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12)))

ts_fc <- ts_monthly_snaive_mod|> forecast(h = "2 years")

ts_fc |> 
  autoplot(ts_monthly_total)

```


### Seasonal naive monthly residuals

```{r, warning=FALSE}
train_snaive_mod <- train |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12)))

mean(augment(train_snaive_mod)$.resid, na.rm=TRUE)  # 85885.5

gg_tsresiduals(train_snaive_mod)
```

Tests to see if residuals differ from white noise:

```{r}
augment(train_snaive_mod) %>% features(.innov, box_pierce, lag = 12)
augment(train_snaive_mod) %>% features(.innov, ljung_box, lag = 12)
```

Takeaways: passes box_pierce, but almost passes ljung_box.  Resids don't look close to normal.  

### Seasonal Naive RMSE

```{r}

fct <-train |> 
  model(model = NAIVE(monthly_total_cyclists ~ lag(12))) |> 
  forecast(test)

(RMSE_seasonal_naive_monthly <-accuracy(fct, ts_monthly_total)$RMSE)

```

RMSE 131213.6	

### Visualize the Seasonal Naive forecast:

```{r message=FALSE, warning=FALSE}
autoplot(train) +
  autolayer(fct, PI = F, size=0.7)+
  autolayer(test)
```

Takeaway:  Errors can get extreme (less than 0 at one point). Does not take into account the trend of the data.  Let's compare to other methods.

Since there is clear seasonality, let's try ETS

## ETS Forecast on monthly data

Method chosen: Holt winters additive (AAA) (with seasonality)
Because the seasonal variations are roughly constant through the series.

```{r}
fit_ETS <- ts_monthly_total %>%
  model(ETS(monthly_total_cyclists ~ error("A") + trend("A") + season("A")))

fc <- fit_ETS |> forecast(h = "2 years")

fc |> 
  autoplot(ts_monthly_total)
```

How does it perform on the test set?

```{r}
fct <- train %>%
  model(ETS(monthly_total_cyclists ~ error("A") + trend("A") + season("A"))) |> 
  forecast(test)

(RMSE_ETS_AAA_monthly <-accuracy(fct, ts_monthly_total)$RMSE)

fct
```

RMSE = 131517.  Worse than seasonal naive.  

### Visualize ETS

```{r}
autoplot(train) +
  autolayer(fct, PI = F, size=0.7)+
  autolayer(test)
```


Maybe the trend was too strong.  I wonder if we can do damping

## ETS with trend damping

```{r}
fct_ETS_AAdA <- train %>%
  model(ETS(monthly_total_cyclists ~ error("A") + trend("Ad") + season("A"))) |> 
  forecast(test)

(RMSE_ETS_AAdA <-accuracy(fct_ETS_AAdA, ts_monthly_total)$RMSE)
```

RMSE: 86652.98.  Much better.  Let's visualize it:

### Vusialize ETS with trend damping

```{r message=FALSE, warnings=FALSE}
autoplot(train) +
  autolayer(fct_ETS_AAdA, size=0.7)+
  autolayer(test)
```

Does it look different when we train it on all of the data and forecast 2 years?

```{r}
fit_ETS <- ts_monthly_total %>%
  model(ETS(monthly_total_cyclists ~ error("A") + trend("Ad") + season("A")))

fc <- fit_ETS |> forecast(h = "2 years")

fc |> 
  autoplot(ts_monthly_total)
```

Just by looking at it, the peak in the summer of 2024 doesn't reach as high.  It's a little more conservative of an estimation.  


## Auto-ETS

What does the ETS command itself decide to be the best model for the training data?

```{r}
mod_Auto_ETS <-train |> 
  model(ETS(monthly_total_cyclists))

report(mod_Auto_ETS)
```

It has chosen a different model, ANA.  Let's see how it does on the test data

```{r}
fct_ETS_ANA <- mod_Auto_ETS |> 
  forecast(test)

(RMSE_ETS_ANA <-accuracy(fct_ETS_ANA, ts_monthly_total)$RMSE)
```

The RMSE is 103125.5.  Not the best.  

How would an ARIMA model fare?

## Auto-ARIMA model 

Let's see what ARIMA model the auto-ARIMA chooses.

```{r}
mod_Auto_ARIMA <- train |> 
  model(ARIMA(monthly_total_cyclists)) 

report(mod_Auto_ARIMA)
```

'Auto-ARIMA' has chosen an ARIMA(1,0,0)(1,1,0)[12] w/ drift model for the train dataset.  Let's see how it scores on the test dataset

```{r}
fct_Auto_ARIMA <- mod_Auto_ARIMA |> 
  forecast(test)

(RMSE_ARIMA_100_110_12 <-accuracy(fct_Auto_ARIMA, ts_monthly_total)$RMSE)
```

138587.7.  Not nearly as good as out best ETS.

Let's see what it's forecast looks like:

```{r message=FALSE, warning=FALSE}
autoplot(train) +
  autolayer(fct_Auto_ARIMA, size=0.7)+
  autolayer(test)
```

## Model Summations

```{r}
cat('seasonal Naive:\t\t\t', RMSE_seasonal_naive_monthly, '\n', 
    'ETS_AAA:\t\t\t', RMSE_ETS_AAA_monthly, '\n',
    'ETS_AAdA:\t\t\t', RMSE_ETS_AAdA, '\n',
    'ETS_ANA:\t\t\t', RMSE_ETS_ANA, '\n',
    'ARIMA (1,0,0)(1,1,0)[12]\t', RMSE_ARIMA_100_110_12)
```


# Test: TS regression

Correlation

```{r}
cor(ts_monthly_total$monthly_total_cyclists, ts_monthly_total$monthly_total_violations)
```

The correlation is very small 

Is there a linear relationship between monthly violations and total cyclists?

```{r}
ts_monthly_total |> 
  ggplot(aes(x=monthly_total_violations, y=monthly_total_cyclists))+
  geom_point()
```

Takeway:  it looks like there are two groups.  My guess is that the groups are pre-covid and post.  Let's just plot post covid:

Post covid cor:

```{r}
ts_monthly_total |> 
  filter_index('2020-04'~.) |> 
  select(monthly_total_violations, monthly_total_cyclists) |> 
  correlate()
```

The correlation is now 0.5012866.  Much better.

Post covid plot:

```{r}
ts_monthly_total |> 
  filter_index('2020-04' ~.) |> 
  ggplot(aes(x=monthly_total_violations , y=monthly_total_cyclists))+
  geom_point()

```

Looks better.  What if we color it by month?

```{r}
ts_monthly_total |> 
  filter_index('2020-04' ~.) |> 
  ggplot(aes(x=monthly_total_violations, y=monthly_total_cyclists, col=as.factor(month(date))))+
  geom_point()
```

Takeaway: Months do seem to cluster together.  



## Ts Reg model (post covid)

Without trend or season:

```{r}
ts_reg_mod1 <- ts_monthly_total |> 
  filter_index('2020-04' ~.) |> 
	model(TSLM(monthly_total_cyclists ~ monthly_total_violations))

report(ts_reg_mod1)
```


Terrible R-squared.

With slope and season:

```{r}
ts_reg_mod2 <- ts_monthly_total |> 
  filter_index('2020-04' ~.) |> 
	model(TSLM(monthly_total_cyclists ~ monthly_total_violations+ trend() + season()))

report(ts_reg_mod2)

```

Takeaway:  The adjusted R^2 is 0.9008, which is very good. June seems to have the highest beta value.  monthly total violations has a tiny beta value, by far.  This shows that seasonality has a larger impact on total cyclists than monthly total violations.


Is monthly total cyclists normally distributed?  

```{r}
qqnorm(ts_monthly_total$monthly_total_cyclists)
```

Ans: Not perfect, but, it's normal enough.


## To do: Can we foreast daily violations?


```{r}

```

